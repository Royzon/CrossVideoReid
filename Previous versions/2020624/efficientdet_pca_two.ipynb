{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import copy\n",
    "import queue\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import itertools\n",
    "import threading\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.backends import cudnn\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess,sirxiapreprocess\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KalmanFilter(object):\n",
    "    def __init__(self,bbox):\n",
    "        (x,y,w,h)=bbox\n",
    "        self.x,self.y=0,0\n",
    "        self.last_measurement = self.current_measurement = np.array((2,1),np.float32)\n",
    "        np_bbox=np.asarray([x,y]).astype(np.float32)\n",
    "        self.last_predicition = self.current_prediction = np_bbox.resize((2,1))#np.zeros((2,1),np.float32)\n",
    "        self.kalman = cv2.KalmanFilter(4,2)\n",
    "        #设置测量矩阵\n",
    "        self.kalman.measurementMatrix = np.array([[1,0,0,0],[0,1,0,0]],np.float32)\n",
    "        #设置转移矩阵\n",
    "        self.kalman.transitionMatrix = np.array([[1,0,1,0],[0,1,0,1],[0,0,1,0],[0,0,0,1]],np.float32)\n",
    "        #设置过程噪声协方差矩阵\n",
    "        self.kalman.processNoiseCov = np.array([[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]],np.float32)*0.03 \n",
    "    def move(self,x,y):\n",
    "        #初始化\n",
    "        self.last_measurement = self.current_measurement\n",
    "        self.last_prediction = self.current_prediction\n",
    "        #传递当前测量坐标值\n",
    "        self.current_measurement = np.array([[np.float32(x)],[np.float32(y)]])\n",
    "        #用来修正卡尔曼滤波的预测结果\n",
    "        self.kalman.correct(self.current_measurement)\n",
    "        # 调用kalman这个类的predict方法得到状态的预测值矩阵，用来估算目标位置\n",
    "        current_prediction = self.kalman.predict()\n",
    "        #上一次测量值\n",
    "        lmx,lmy = self.last_measurement[0],self.last_measurement[1]\n",
    "        #当前测量值\n",
    "        cmx,cmy = self.current_measurement[0],self.current_measurement[1]\n",
    "        #上一次预测值\n",
    "        #lpx,lpy = last_prediction[0],last_prediction[1]\n",
    "        #当前预测值\n",
    "        cpx,cpy = current_prediction[0],current_prediction[1]\n",
    "        return cpx,cpy\n",
    "    def update(self,bbox):\n",
    "        (x,y,w,h)=bbox\n",
    "        self.x,self.y = self.move(x,y)\n",
    "    def get(self):\n",
    "        return self.x,self.y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class efficientdet:\n",
    "    def __init__(self,compound_coef,force_input_size = 1920,huamanNum=1,dir_path='./walkingworkspace/train_all',NUM_EIGEN_FACES= 128,test=False):\n",
    "        self.humanNum=huamanNum\n",
    "        cudnn.fastest = True\n",
    "        cudnn.benchmark = True\n",
    "        self.bbox_save=[]\n",
    "        self.id_save=[]\n",
    "        self.test=test\n",
    "        self.bbox_threshold=0.01#%bbox的阈值!<\n",
    "        self.probability_threshold=0.3#%人体识别概率的阈值!<\n",
    "        self.use_cuda = True \n",
    "        self.use_float16 = False\n",
    "        self.threshold = 0.2\n",
    "        self.iou_threshold = 0.2\n",
    "        self.obj_list = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "            'fire hydrant', '', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep',\n",
    "            'cow', 'elephant', 'bear', 'zebra', 'giraffe', '', 'backpack', 'umbrella', '', '', 'handbag', 'tie',\n",
    "            'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
    "            'skateboard', 'surfboard', 'tennis racket', 'bottle', '', 'wine glass', 'cup', 'fork', 'knife', 'spoon',\n",
    "            'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut',\n",
    "            'cake', 'chair', 'couch', 'potted plant', 'bed', '', 'dining table', '', '', 'toilet', '', 'tv',\n",
    "            'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',\n",
    "            'refrigerator', '', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
    "            'toothbrush']\n",
    "        #self.device = torch.device('cuda')\n",
    "        self.model = EfficientDetBackbone(compound_coef=compound_coef, num_classes=len(self.obj_list))\n",
    "        self.model.load_state_dict(torch.load(f'weights/efficientdet-d{compound_coef}.pth'))\n",
    "        self.model.requires_grad_(False)\n",
    "        self.model.eval()\n",
    "        #self.model=nn.DataParallel(self.model)\n",
    "        self.model.cuda()\n",
    "        input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536]\n",
    "        self.input_size =  force_input_size\n",
    "        self.t=time.time()\n",
    "        self.pca=PCA(dir_path,NUM_EIGEN_FACES)\n",
    "        if self.test==True:\n",
    "            self.pca.test('./walkingworkspace/test2020')\n",
    "            self.label_test=[]\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        name = time.strftime('%Y.%m.%d',time.localtime(time.time()))\n",
    "        self.out = cv2.VideoWriter('./savevideo/'+name+'.avi',fourcc, 30.0, (640,480))\n",
    "        self.save={}\n",
    "        self.frame=0\n",
    "    def init_video(self):\n",
    "        self.out.release()\n",
    "        self.pca.clear_id()\n",
    "        self.pca.frame=0\n",
    "        name = time.strftime('%Y.%m.%d',time.localtime(time.time()))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        self.out = cv2.VideoWriter('./savevideo/'+name+'.avi',fourcc, 30.0, (640,480))\n",
    "    def threadStart(self):\n",
    "        threadone=threading.Thread(target=self.mythread,args=())\n",
    "        threadone.start()\n",
    "      \n",
    "    def mythread(self):\n",
    "        global data_queue,out_queue\n",
    "        print('start thread ->>> efficientdet ')\n",
    "        while True:\n",
    "            self.t=time.time()\n",
    "            while(data_queue.qsize()<2):\n",
    "                time.sleep(0.01)\n",
    "                if time.time()-self.t>10:\n",
    "                    print('stop thread ->>> efficientdet ')\n",
    "                    break\n",
    "            out_queue.put(self.detector(data_queue.get()))\n",
    "                            ##################\n",
    "                            ####detector######\n",
    "                            ##################\n",
    "    def  detector(self,im):\n",
    "        ori_imgs, framed_imgs, framed_metas = sirxiapreprocess(im, max_size=self.input_size)\n",
    "        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "        x = x.to(torch.float32 if not self.use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "        with torch.no_grad():\n",
    "            features, regression, classification, anchors = self.model(x)\n",
    "            regressBoxes = BBoxTransform()\n",
    "            clipBoxes = ClipBoxes()\n",
    "            out = postprocess(x,\n",
    "                              anchors, regression, classification,\n",
    "                              regressBoxes, clipBoxes,\n",
    "                              self.threshold, self.iou_threshold)\n",
    "            out = invert_affine(framed_metas, out)\n",
    "            bbox_list,c_list=self.update(out, ori_imgs)\n",
    "            #print(bbox_list,c_list)\n",
    "            #identities,bbox_xyxy = DeepSort.deep_sort(np.asarray(bbox_list),np.asarray(c_list),im)\n",
    "            identities,dths,color = self.pca.efficientdet_compute_pca(bbox_list,c_list,im)        \n",
    "            im =self.display(bbox_list,c_list ,im,identities,dths,color) \n",
    "            #for bbox,id_ in zip(bbox_list,identities):\n",
    "                #self.bbox_save.append(bbox)\n",
    "                #self.id_save.append(id_)\n",
    "            if self.test:\n",
    "                self.save[self.frame]=[identities,bbox_list]\n",
    "            self.out.write(im)\n",
    "            self.t=time.time()\n",
    "            cv2.waitKey(1)\n",
    "            self.frame+=1\n",
    "            #return identities,bbox_xyxy,c_list\n",
    "            #return bbox_list,c_list ,identities\n",
    "    #过滤\n",
    "    def update(self,preds, imgs):\n",
    "        save_list=[]\n",
    "        area_list=[]\n",
    "        score_list=[]\n",
    "        for i in range(len(imgs)):\n",
    "            if len(preds[i]['rois']) == 0:\n",
    "                continue\n",
    "            for j in range(len(preds[i]['rois'])):\n",
    "                if preds[i]['class_ids'][j]==0:#person\n",
    "                    score = float(preds[i]['scores'][j])#百分比\n",
    "                    H,W=imgs[i].shape[:2]\n",
    "                    (x1, y1, x2, y2) = preds[i]['rois'][j].astype(np.int)\n",
    "                    area = (x2-x1)*(y2-y1)\n",
    "                    if  (area/H*W)<self.bbox_threshold or score<self.probability_threshold:\n",
    "                        continue\n",
    "                    x,y,w,h=(x1+x2)/2,(y1+y2)/2,abs(x1-x2),abs(y1-y2)\n",
    "                    area_list.append(area)\n",
    "                    save_list.append([x,y,w,h]) \n",
    "                    score_list.append(score)\n",
    "        bbox_list,c_list=[],[]\n",
    "        if len(area_list)>self.humanNum:\n",
    "            \n",
    "            for i in range(self.humanNum):\n",
    "                one=save_list[area_list.index(max(area_list))]\n",
    "                two=score_list[area_list.index(max(area_list))]\n",
    "                bbox_list.append(copy.copy(one))\n",
    "                c_list.append(copy.copy(two))\n",
    "                save_list.remove(one)\n",
    "                score_list.remove(two)\n",
    "                area_list.remove(max(area_list))\n",
    "                 \n",
    "        else:\n",
    "            for i in range(len(area_list)):\n",
    "                one=save_list[area_list.index(max(area_list))]\n",
    "                two=score_list[area_list.index(max(area_list))]\n",
    "                bbox_list.append(copy.copy(one))\n",
    "                c_list.append(copy.copy(two))\n",
    "                save_list.remove(one)\n",
    "                score_list.remove(two)\n",
    "                area_list.remove(max(area_list))\n",
    "        return bbox_list,c_list\n",
    "    \n",
    "    def display(self,bbox_list,c_list,im,identities,dths,color):\n",
    "        #print(len(out_list))\n",
    "        for bbox,c,id_,dth,color_ in zip(bbox_list,c_list,identities,dths,color):\n",
    "            #(x1, y1, x2, y2) = bbox\n",
    "            (x, y, w, h) = bbox\n",
    "            (x1, y1, x2, y2)=int(x-w/2), int(y-h/2), int(x+w/2), int(y+h/2)\n",
    "            cv2.rectangle(im, (x1, y1), (x2, y2), (0, color_, 255-color_), 2)\n",
    "            #cv2.putText(im, ('%.2f'%c), (x2+10, y1+10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 250,0), 1)\n",
    "            cv2.putText(im, ('%s'%id_), (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, color_, 255-color_), 1.5)\n",
    "            cv2.putText(im, ('%.2f'%dth), (x2, y1+15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, color_, 255-color_), 1)\n",
    "        fps = 1/(time.time()-self.t)\n",
    "        cv2.putText(im, ('%.2f'%fps), (590, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)\n",
    "        cv2.imshow('efficientdet', im)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import fcn_resnet101\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "class PCA:\n",
    "    def __init__(self,dir_path='./test_colors/train',NUM_EIGEN_FACES = 64):\n",
    "        self.save={'000':{'center':[],'bbox':[],'lib':[],'frame':0,'passage':False,'kalman':[],'cross':False}}\n",
    "        self.NUM_EIGEN_FACES = NUM_EIGEN_FACES\n",
    "        self.images = self.readImages(dir_path)\n",
    "        self.data = self.createDataMatrix(self.images)\n",
    "        #self.data = torch.from_numpy(self.data).cuda()\n",
    "        #torch.pca_lowrank(A, q=None, center=True, niter=2)\n",
    "        #self.mean, self.eigenVectors = torch.pca_lowrank(self.data, q=None, center=True, niter= self.NUM_EIGEN_FACES)\n",
    "        #self.means, self.eigenVectorses=[],[]\n",
    "        #print('the alldata %d'%len(self.eigenVectors))\n",
    "        #self.init_pca(basepath)\n",
    "        #self.notebook={'000':{'frame':[],'lib':[]}}\n",
    "        self.frame=0\n",
    "        self.color_old=0\n",
    "        self.flag_frame=[]\n",
    "        self.flag_line=[]\n",
    "        self.dth = 0.5#相似度距离0.6  / 欧氏距离25\n",
    "        self.Forget=30  #状态切换帧\n",
    "        self.testsave={}\n",
    "        self.models_mean = './models/mean.npy'\n",
    "        self.models_eigenVector = './models/eigenVector.npy'\n",
    "        #self.meta = threading.Lock()\n",
    "        if os.path.exists(self.models_mean)==False and os.path.exists(self.models_eigenVector)==False:\n",
    "            self.mean, self.eigenVectors = cv2.PCACompute(self.data, mean=None, maxComponents=self.NUM_EIGEN_FACES)\n",
    "            np.save(self.models_mean,self.mean)\n",
    "            np.save(self.models_eigenVector,self.eigenVectors)\n",
    "        else:\n",
    "            self.mean=np.load(self.models_mean)\n",
    "            self.eigenVectors=np.load(self.models_eigenVector)\n",
    "        #卡尔曼滤波\n",
    "        self.kalman=[]\n",
    "    def createDataMatrix(self,images):\n",
    "        numImages = len(images)\n",
    "        sz = images[0].shape\n",
    "        data = np.zeros((numImages, sz[0] * sz[1] * sz[2]), dtype=np.float32)\n",
    "        for i in range(0, numImages):\n",
    "            image = images[i].flatten()\n",
    "            data[i,:] = image\n",
    "        #print(\"createData ok\")\n",
    "        return data\n",
    "\n",
    "    def readImages(self,path):\n",
    "        print(\"Reading images from \" + path, end=\"...\")\n",
    "        # Create array of array of images.\n",
    "        images = []\n",
    "        # List all files in the directory and read points from text files one by one\n",
    "        for name in glob.glob(path+'/*'):\n",
    "            for imagePath in glob.glob(name+'/*.jpg'):\n",
    "                    # Add to array of images\n",
    "                    im = cv2.imread(imagePath)\n",
    "                    im = cv2.resize(im,(64,128))\n",
    "                    if im is None :\n",
    "                        print(\"image:{} not read properly\".format(imagePath))\n",
    "                    else :\n",
    "                        # Convert image to floating point\n",
    "                        im = np.float32(im)/255.0\n",
    "                        # Add image to list\n",
    "                        images.append(im)\n",
    "                        # Flip image \n",
    "                        imFlip = cv2.flip(im, 1);\n",
    "                        # Append flipped image\n",
    "                        #images.append(imFlip)\n",
    "        numImages = int(len(images))\n",
    "        # Exit if no image found\n",
    "        if numImages == 0 :\n",
    "            print(\"No images found\")\n",
    "            sys.exit(0)\n",
    "        print(str(numImages) + \" files read.\")\n",
    "        return images\n",
    "#*\n",
    "#*\n",
    "#*      算法集合\n",
    "#*\n",
    "#*\n",
    "    def compute_cos(self,x,y):\n",
    "        x_,y_=x.flatten(),y.flatten()\n",
    "        dist =1- abs(np.dot(x_,y_)/(np.linalg.norm(x_)*np.linalg.norm(y_)))      \n",
    "        return abs(dist)\n",
    "    \n",
    "    def compute_dis(self,x,y):\n",
    "         return np.linalg.norm( x - y )\n",
    "    \n",
    "    def compute_Maha(self,x,y):\n",
    "        X=np.vstack([x,y])\n",
    "        XT=X.T\n",
    "        S=np.cov(X)   #两个维度之间协方差矩阵\n",
    "        SI = np.linalg.inv(S) #协方差矩阵的逆矩阵\n",
    "        #马氏距离计算两个样本之间的距离，此处共有10个样本，两两组合，共有45个距离。\n",
    "        n=XT.shape[0]\n",
    "        d1=[]\n",
    "        for i in range(0,n):\n",
    "            for j in range(i+1,n):\n",
    "                delta=XT[i]-XT[j]\n",
    "                d=np.sqrt(np.dot(np.dot(delta,SI),delta.T))\n",
    "                d1.append(d)\n",
    "        return d1\n",
    "    \n",
    "    def computer_Hungary(self,task_matrix):\n",
    "        b = task_matrix.copy()\n",
    "        # 行和列减0\n",
    "        for i in range(len(b)):\n",
    "            row_min = np.min(b[i])\n",
    "            for j in range(len(b[i])):\n",
    "                b[i][j] -= row_min\n",
    "        for i in range(len(b[0])):\n",
    "            col_min = np.min(b[:, i])\n",
    "            for j in range(len(b)):\n",
    "                b[j][i] -= col_min\n",
    "        line_count = 0\n",
    "        # 线数目小于矩阵长度时，进行循环\n",
    "        while (line_count < len(b)):\n",
    "            line_count = 0\n",
    "            row_zero_count = []\n",
    "            col_zero_count = []\n",
    "            for i in range(len(b)):\n",
    "                row_zero_count.append(np.sum(b[i] == 0))\n",
    "            for i in range(len(b[0])):\n",
    "                col_zero_count.append((np.sum(b[:, i] == 0)))\n",
    "            # 划线的顺序（分行或列）\n",
    "            line_order = []\n",
    "            row_or_col = []\n",
    "            for i in range(len(b[0]), 0, -1):\n",
    "                while (i in row_zero_count):\n",
    "                    line_order.append(row_zero_count.index(i))\n",
    "                    row_or_col.append(0)\n",
    "                    row_zero_count[row_zero_count.index(i)] = 0\n",
    "                while (i in col_zero_count):\n",
    "                    line_order.append(col_zero_count.index(i))\n",
    "                    row_or_col.append(1)\n",
    "                    col_zero_count[col_zero_count.index(i)] = 0\n",
    "            # 画线覆盖0，并得到行减最小值，列加最小值后的矩阵\n",
    "            delete_count_of_row = []\n",
    "            delete_count_of_rol = []\n",
    "            row_and_col = [i for i in range(len(b))]\n",
    "            for i in range(len(line_order)):\n",
    "                if row_or_col[i] == 0:\n",
    "                    delete_count_of_row.append(line_order[i])\n",
    "                else:\n",
    "                    delete_count_of_rol.append(line_order[i])\n",
    "                c = np.delete(b, delete_count_of_row, axis=0)\n",
    "                c = np.delete(c, delete_count_of_rol, axis=1)\n",
    "                line_count = len(delete_count_of_row) + len(delete_count_of_rol)\n",
    "                # 线数目等于矩阵长度时，跳出\n",
    "                if line_count == len(b):\n",
    "                    break\n",
    "                # 判断是否画线覆盖所有0，若覆盖，进行加减操作\n",
    "                if 0 not in c:\n",
    "                    row_sub = list(set(row_and_col) - set(delete_count_of_row))\n",
    "                    min_value = np.min(c)\n",
    "                    for i in row_sub:\n",
    "                        b[i] = b[i] - min_value\n",
    "                    for i in delete_count_of_rol:\n",
    "                        b[:, i] = b[:, i] + min_value\n",
    "                    break\n",
    "        row_ind, col_ind = linear_sum_assignment(b)\n",
    "        min_cost = task_matrix[row_ind, col_ind].sum()\n",
    "        best_solution = list(task_matrix[row_ind, col_ind])\n",
    "        return  best_solution\n",
    "     #计算两点近距离公式 xyxy\n",
    "    def distEclud(self,veA,vecA,veB,vecB):\n",
    "        lossA=veB-veA\n",
    "        lossB=vecB-vecA\n",
    "        return math.sqrt(pow(lossA,2)+pow(lossB,2))\n",
    "#*\n",
    "#*\n",
    "#*      功能函数\n",
    "#*\n",
    "#*\n",
    "    def clear_id(self):\n",
    "        self.save={'000':{'center':[],'bbox':[],'lib':[],'frame':0,'passage':False,'kalman':[],'cross':False}}   \n",
    "        \n",
    "    def kalman_reid(self,bbox):\n",
    "        (x,y,w,h)=bbox\n",
    "        list_kalman_distance,list_kalman_id=[],[]\n",
    "        for k,v in list(self.save.items()):\n",
    "            if k!='000'and len(v['lib'])>10:\n",
    "                list_kalman_distance.append(self.distEclud(x,y,v['kalman'][0],v['kalman'][1]))\n",
    "                list_kalman_id.append(k)\n",
    "        if len(list_kalman_id)>0:\n",
    "            id_ = list_kalman_id[list_kalman_distance.index(min(list_kalman_distance))]\n",
    "            #self.save[id_]['cross']=True\n",
    "        else:\n",
    "            id_ = ' '\n",
    "        return id_\n",
    "    def manage_lib(self,frame):\n",
    "        for k,v in list(self.save.items()):\n",
    "            if k=='000':#or v['passage']==False:\n",
    "                continue\n",
    "            lib = v['lib']\n",
    "            frame_lib=v['frame']\n",
    "            if frame-frame_lib>self.Forget:\n",
    "                self.save[k]['frame']=frame\n",
    "                #self.save[k]['lib'].pop(0)\n",
    "                if len(lib)<10:\n",
    "                    self.dele_lib_id(k)\n",
    "                if  k not in self.save:\n",
    "                    continue\n",
    "                if self.save[k]['passage']==True:\n",
    "                    center =self.save[k]['center']\n",
    "                    for lib_ in self.save[k]['lib']:\n",
    "                        center=center*0.99+lib_*0.01\n",
    "                    #self.save[k]['lib'].pop(0)\n",
    "                    self.passage_off(k,center)\n",
    "                    #self.flag_frame=[]\n",
    "                    \n",
    "    def paassag_on(self,feature,bbox,frame,id_):\n",
    "        self.save[id_]['center']=feature\n",
    "        self.save[id_]['bbox']=bbox\n",
    "        self.save[id_]['lib'].append(feature)\n",
    "        self.save[id_]['frame']=frame\n",
    "        self.save[id_]['passage']=True\n",
    "        print('%s passage is True'%id_)\n",
    "        \n",
    "    def passage_off(self,id_,center):\n",
    "        self.save[id_]['center']=center\n",
    "        self.save[id_]['passage']=False\n",
    "        print('%s passage is False'%id_)\n",
    "        \n",
    "    def update_frame(self,id_,frame):\n",
    "        self.save[id_]['frame']=frame\n",
    "        \n",
    "    def dele_lib_id(self,id_):\n",
    "        del self.save[id_]\n",
    "        print('the id %s Remove'%id_)\n",
    "        \n",
    "    def add_lib(self,feature,bbox,frame,passage=False):\n",
    "        save={'center':[],'bbox':[],'lib':[],'frame':0,'passage':False,'kalman':[],'cross':False}\n",
    "        for n in  self.save.keys():\n",
    "            id_=n\n",
    "        self.kalman.append(KalmanFilter(bbox))\n",
    "        self.kalman[(int(id_))].update(bbox)\n",
    "        x,y=self.kalman[(int(id_))].get()\n",
    "        \n",
    "        save['kalman']=[x,y]\n",
    "        self.id_new=('%03d'%(int(id_)+1))\n",
    "        save['center']=feature\n",
    "        save['bbox']=bbox\n",
    "        save['lib']=[feature]\n",
    "        save['frame']=frame\n",
    "        save['passage']=passage\n",
    "        self.save[self.id_new]=save\n",
    "        print('find id %s'%self.id_new)\n",
    "        return self.id_new\n",
    "    \n",
    "    def add_id(self,feature,bbox,frame):   \n",
    "        #print(len(self.save))\n",
    "        if frame!=0 and len(self.save)!=1:\n",
    "            id_ = self.check_lib_passage(feature)\n",
    "            #self.id_new = self.check_lib(feature)\n",
    "            if id_ == ' ':\n",
    "                self.flag_line=[]\n",
    "                if self.id_new in self.save:\n",
    "                    if frame-self.save[self.id_new]['frame']>1:\n",
    "                        self.flag_frame=[]\n",
    "                else:\n",
    "                    self.flag_frame=[]\n",
    "                if len(self.flag_frame)==0:\n",
    "                    self.id_new = self.add_lib(feature,bbox,frame)\n",
    "                self.flag_frame.append(frame)\n",
    "                self.save[self.id_new]['frame']=frame\n",
    "                #self.update_frame\n",
    "                if len(self.flag_frame)>=5:\n",
    "                    if np.mean(self.flag_frame)==self.flag_frame[2]:\n",
    "                        self.paassag_on(feature,bbox,frame,self.id_new)\n",
    "                        self.flag_frame=[]\n",
    "                    else:\n",
    "                        self.dele_lib_id(self.id_new)\n",
    "                        self.flag_frame=[]\n",
    "            else:\n",
    "                #if frame - self.save[id_]['frame'] >3:\n",
    "                self.flag_frame=[]\n",
    "                self.flag_line.append(frame)\n",
    "                self.save[id_]['frame']=frame\n",
    "                #self.update_frame\n",
    "                if len(self.flag_line)>=3:\n",
    "                    if np.mean(self.flag_line)==self.flag_line[1]:\n",
    "                        self.paassag_on(feature,bbox,frame,id_)\n",
    "                        self.flag_line=[]\n",
    "                    else:\n",
    "                        self.flag_line=[]\n",
    "                return id_\n",
    "        else:\n",
    "            self.id_new = self.add_lib(feature,bbox,frame,passage=True)\n",
    "        return self.id_new\n",
    "#*\n",
    "#*      检测集合\n",
    "#*                        \n",
    "    def check_frame_id(self,id_,feature,frame):\n",
    "        list_dis=[]\n",
    "        for k,v in list(self.save.items()):\n",
    "            if k==id_:\n",
    "                if frame-(v['frame'])>=10:\n",
    "                        if k==id_:\n",
    "                            for lib_ in v['lib']:\n",
    "                                list_dis.append(self.compute_cos(lib_,feature))\n",
    "                            if min(list_dis)<self.dth:\n",
    "                                return True\n",
    "                            else:\n",
    "                                return False\n",
    "                else:\n",
    "                    return True\n",
    "        return False\n",
    "    #检查除id_me外是否与数据库里的bbox重叠\n",
    "    def check_bboxs(self,bbox_list,bbox):    \n",
    "        (xc, yc, wc, hc) =bbox\n",
    "        list_id,list_x=[],[]\n",
    "        ret=True\n",
    "        if len(bbox_list)!=1:\n",
    "        #if (5<(xc-(wc*0.5)) and(xc+(wc*0.5))<(im_w-10)) or (5<(yc-(hc*0.5)) and (yc+(hc*0.5))<(im_h-10)):\n",
    "            for i in range(len(bbox_list)):\n",
    "                for j in range(i+1,len(bbox_list)):\n",
    "                    (xa, ya, wa, ha) = bbox_list[i]\n",
    "                    (xb, yb, wb, hb) = bbox_list[j] \n",
    "                    distance = self.distEclud(xa,ya,xb,yb)\n",
    "                    if (distance/(wa+wb))>=0.5:\n",
    "                        continue\n",
    "                    if (distance/(wa+wb))<0.5:\n",
    "                        if xc==xa or xc==xb:#是不是因为重叠导致的\n",
    "                            ret=False\n",
    "        return ret\n",
    "    def check_bboxs_cross(self,bboxs,bbox):    \n",
    "        ret=True\n",
    "        (xa, ya, wa, ha)=bbox\n",
    "        cross_bboxs=copy.copy(bboxs)\n",
    "        cross_bboxs.remove(bbox)\n",
    "        for bbox_ in cross_bboxs:\n",
    "            (xb, yb, wb, hb) = bbox_ \n",
    "            distance = self.distEclud(xa,ya,xb,yb)\n",
    "            if (distance/(wa+wb))>=0.5:\n",
    "                continue\n",
    "            if (distance/(wa+wb))<0.5:\n",
    "                ret=False\n",
    "        return ret\n",
    "    def check_bboxs_overlap(self,bboxs,bbox):\n",
    "        ret=True\n",
    "        over_bboxs=copy.copy(bboxs)\n",
    "        over_bboxs.remove(bbox)\n",
    "        for bbox in zip(over_bboxs):\n",
    "            (xc, yc, wc, hc)=bbox\n",
    "            for k,v in list(self.save.items()):\n",
    "                if v['passage']==True and k!='000'and len(v['lib'])>10:\n",
    "                    (xl, yl, wl, hl) = v['bbox']\n",
    "                    distance = self.distEclud(xc,yc,xl,yl)\n",
    "                    if (distance/(wc+wl))>=0.5:\n",
    "                        continue\n",
    "                    if (distance/(wc+wl))<0.5:  \n",
    "                        ret=False\n",
    "        return ret\n",
    "    def check_lib_add(self,featureVector,min_distance_id,bbox):\n",
    "        for k,v in list(self.save.items()):\n",
    "            list_dis=[]\n",
    "            if  k==min_distance_id:\n",
    "                n=v['center']\n",
    "                lib = v['lib']\n",
    "                #跟新kalman 轨迹预测\n",
    "                kalman_id=int(k)-1\n",
    "                self.kalman[kalman_id].update(bbox)\n",
    "                x,y =self.kalman[kalman_id].get()\n",
    "                self.save[k]['kalman']=[x,y]\n",
    "                for lib_ in lib:\n",
    "                    list_dis.append(self.compute_cos(lib_,featureVector))\n",
    "                if len(self.save[min_distance_id]['lib'])>511:\n",
    "                    num = list_dis.index(min(list_dis))\n",
    "                    self.save[min_distance_id]['lib'].pop(num)\n",
    "                    #self.save[min_distance_id]['lib'].pop(0)\n",
    "                if len(list_dis)>0 and np.mean(list_dis)>self.dth:\n",
    "                    self.save[min_distance_id]['lib'].append(featureVector)\n",
    "                    print('\\r%s lib len :%d'%(min_distance_id,len(lib)),end='')\n",
    "                    #跟新kalman bbox\n",
    "                    num=int(k)-1\n",
    "                    self.kalman[num].update(bbox)\n",
    "                break\n",
    "        return int(len(lib)/2)\n",
    "    #检查就绪的\n",
    "    def check_lib(self,featureVector):\n",
    "        list_id,list_mean=[],[]\n",
    "        for k,v in list(self.save.items()):\n",
    "            list_dis=[]\n",
    "            if k=='000'or v['passage']==False :\n",
    "                continue\n",
    "            lib = v['lib']\n",
    "            for lib_ in lib:\n",
    "                list_dis.append(self.compute_cos(lib_,featureVector))\n",
    "            list_id.append(k)\n",
    "            list_mean.append(min(list_dis)) \n",
    "        if len(list_mean)>0 and min(list_mean) <self.dth:\n",
    "            return list_id[list_mean.index(min(list_mean))] \n",
    "        else:\n",
    "            #print( min(list_mean))\n",
    "            return ' '   \n",
    "    #检查未就绪\n",
    "    def check_lib_passage(self,featureVector):\n",
    "        list_id,list_mean=[],[]\n",
    "        for k,v in list(self.save.items()):\n",
    "            list_dis=[]\n",
    "            if k!='000' and v['passage']==False and len(v['lib'])>10:\n",
    "                lib = v['lib']\n",
    "                for lib_ in lib:\n",
    "                    list_dis.append(self.compute_cos(lib_,featureVector))\n",
    "                list_id.append(k)\n",
    "                list_mean.append(min(list_dis)) \n",
    "        if len(list_mean)>0 and min(list_mean) <self.dth:\n",
    "            return list_id[list_mean.index(min(list_mean))] \n",
    "        else:\n",
    "            #print( min(list_mean))\n",
    "            return ' '    \n",
    "   \n",
    "    #def efficientdet_compute_pca(self,bbox_list,confidences,im):\n",
    "    def efficientdet_compute_pca(self,bbox_list,confidences,im):\n",
    "        #\n",
    "        # * self.notebook 记录发现新的图片的W\n",
    "        # * 保存数据的格式 self.save={'id':{'center':w,'LIB':[w1..wn]}}\n",
    "        #\n",
    "        color=[]\n",
    "        identities=[]\n",
    "        dths=[]\n",
    "        #维护LIB\n",
    "        self.manage_lib(self.frame)\n",
    "        if bbox_list!=[]:\n",
    "            list_id,list_feature,list_distance,list_center=[],[],[],[]\n",
    "            user_bbox_list=copy.copy(bbox_list)\n",
    "            for bbox,c in zip(user_bbox_list,confidences):\n",
    "                (x, y, w, h) = bbox\n",
    "                (x1, y1, x2, y2)=int(x-w/2), int(y-h/2), int(x+w/2), int(y+h/2)\n",
    "                #(x,y,w,h)=(x1+x2)*0.5,(y1+y2)*0.5,abs(x1-x2),abs(y1-y2)\n",
    "                im_=im[y1:y2,x1:x2]\n",
    "                im_ = cv2.resize(im_,(64,128))\n",
    "                featureVector = self.compute_feature(im_)\n",
    "                #交叉判断\n",
    "                #print(bbox_list)\n",
    "                ret_bbox_cross = self.check_bboxs_cross(user_bbox_list,bbox)\n",
    "                #ret_bbox_overlap=self.check_bboxs_overlap(bbox_list,bbox)\n",
    "                #有交叉则使用kalman跟踪\n",
    "                if ret_bbox_cross ==False :#or ret_bbox_overlap==False:\n",
    "                    bbox_list.remove(bbox)\n",
    "                    id_ = self.kalman_reid(bbox) \n",
    "                    print(id_)\n",
    "                    if id_==' ':\n",
    "                        id_ = self.add_id(featureVector,bbox,self.frame)\n",
    "                        identities.append(id_)\n",
    "                        dths.append(c)\n",
    "                        color.append(0)\n",
    "                    else:\n",
    "                        self.save[id_]['bbox'] =bbox\n",
    "                        self.save[id_]['frame'] =self.frame\n",
    "                        dths.append(c)\n",
    "                        identities.append(id_)\n",
    "                        color_=self.check_lib_add(featureVector,id_,bbox)\n",
    "                        color.append(color_)\n",
    "                #PCA算法\n",
    "                else:\n",
    "                    distance=0\n",
    "                #所有图片与LIB中passage为TRUE的计算距离\n",
    "                    list_distance_buffer,list_id_buffer,list_feature_buffer,list_center_buffer=[],[],[],[]\n",
    "                    for k,v in list(self.save.items()):\n",
    "                        if k=='000':\n",
    "                            continue\n",
    "                        if v['passage']==True:\n",
    "                            n=v['center']\n",
    "                            #kalman_xy=v['kalman']\n",
    "                            #distance_kalman = self.distEclud(kalman_xy[0],kalman_xy[1],bbox[0],bbox[1])\n",
    "                            #print(distance_kalman)\n",
    "                            distance = self.compute_cos(n,featureVector)\n",
    "                            #distance =+distance_feature*0.95+distance_kalman*0.005\n",
    "                            list_distance_buffer.append(distance)\n",
    "                            list_id_buffer.append(k)\n",
    "                            list_feature_buffer.append(featureVector)\n",
    "                            list_center_buffer.append(n) \n",
    "\n",
    "                    if len(list_distance_buffer)>0 :#and min(list_distance_buffer)<self.dth:\n",
    "                        list_distance.append(list_distance_buffer)\n",
    "                        list_id.append(list_id_buffer)\n",
    "                        list_feature.append(list_feature_buffer)\n",
    "                        list_center.append(list_center_buffer)\n",
    "                    else:\n",
    "                        #ret = self.check_bboxs(bbox_list,bbox) \n",
    "                        #if ret ==True:\n",
    "                        id_ = self.add_id(featureVector,bbox,self.frame)\n",
    "                        identities.append(id_)\n",
    "                        dths.append(c)\n",
    "                        color.append(0) \n",
    "            #匈牙利算法\n",
    "            list_bbox_loss,list_feature_loss=[],[]\n",
    "            if list_distance!=[]:\n",
    "                    kalman_center,kalman_feature,kalman_id,kalman_bbox=[],[],[],[]\n",
    "                    kalman_bbox=bbox_list\n",
    "                    #需要id的数量要小于等于就绪id的数量\n",
    "                    #print(len(bbox_list),len(list_distance[0]))\n",
    "                    if len(bbox_list)>len(list_distance[0]):\n",
    "                        min_distance=[]\n",
    "                        #print(list_distance,bbox_list)\n",
    "                        for id_distanc in list_distance:\n",
    "                            min_distance.append(min(id_distanc))\n",
    "                        for i in range((len(bbox_list)-len(list_distance[0]))):\n",
    "                            num=min_distance.index(max(min_distance))\n",
    "                            min_distance.pop(num)\n",
    "                            list_distance.pop(num)\n",
    "                            list_id.pop(num)\n",
    "                            list_center.pop(num)  \n",
    "                            list_feature_loss.append(list_feature.pop(num))\n",
    "                            list_bbox_loss.append(kalman_bbox.pop(num))\n",
    "                            #print(list_bbox_loss,list_feature_loss)\n",
    "                            confidences.pop(num)\n",
    "                    if list_distance==[]:\n",
    "                        return identities,dths,color\n",
    "                    np_distance=np.asarray(list_distance)\n",
    "                    #print(np_distance)\n",
    "                    #print(self.save.keys())\n",
    "                    list_Hungary = self.computer_Hungary(np_distance)\n",
    "                    #print(list_Hungary)\n",
    "                    list_id_,list_center_,list_feature_=[],[],[]\n",
    "                    for id_s,center_s,feature_s in zip(list_id,list_center,list_feature):\n",
    "                        for id_,center_,feature_ in zip(id_s,center_s,feature_s):\n",
    "                            list_id_.append(id_)\n",
    "                            list_center_.append(center_)\n",
    "                            list_feature_.append(feature_)\n",
    "                    for number in list_Hungary:\n",
    "                        #number = number*0.00001               \n",
    "                        min_distance = np_distance.flatten().tolist().index(number)\n",
    "                        kalman_id.append(list_id_[min_distance])\n",
    "                        kalman_center.append(list_center_[min_distance])\n",
    "                        kalman_feature.append(list_feature_[min_distance])\n",
    "\n",
    "                    #检查重新回来的id的frame\n",
    "                    #ret_id_frame = self.check_frame_id(min_distance_id,min_distance_feature,self.frame)\n",
    "                    #print(kalman_bbox,kalman_center,kalman_feature,kalman_id,confidences)\n",
    "                    #ret_bbox_cross = self.check_bboxs_cross(kalman_bbox,kalman_id)\n",
    "                    #if ret_bbox_cross==False :\n",
    "                        #kalman_bbox=self.kalman_reid(kalman_bbox,kalman_id)\n",
    "                    for bbox,conter,feature,id_,c in zip(kalman_bbox,kalman_center,kalman_feature,kalman_id,confidences):\n",
    "                         #检查除id_me外是否与数据库里的bbox重叠\n",
    "                        center_ = (conter*0.5)+(feature*0.5)\n",
    "                        self.save[id_]['center'] =center_\n",
    "                        self.save[id_]['bbox'] =bbox\n",
    "                        self.save[id_]['frame'] =self.frame\n",
    "                        color_=self.check_lib_add(feature,id_,bbox)\n",
    "                        color.append(color_)\n",
    "                        dths.append(c)\n",
    "                        identities.append(id_)\n",
    "                    #没有ID,重新add_id    \n",
    "                    #print(list_bbox_loss,list_feature_loss,confidences)\n",
    "                    for bbox,featureVector,c in zip(list_bbox_loss,list_feature_loss,confidences):\n",
    "                        #print(bbox,featureVector,c)\n",
    "                        id_ = self.add_id(featureVector[0],bbox,self.frame)\n",
    "                        identities.append(id_)\n",
    "                        dths.append(c)\n",
    "                        color.append(0)  \n",
    "        self.frame+=1\n",
    "        return identities,dths,color\n",
    "    #输入图片计算，输出Feature\n",
    "    def compute_feature(self,im):\n",
    "        im_ = im\n",
    "        size=im_.shape\n",
    "        im_ = np.float32(im_)/255.0\n",
    "        Fim = im_.flatten()\n",
    "        Fmean = self.mean.reshape(Fim.shape)\n",
    "        Fdf = Fim-Fmean\n",
    "        Fdf= Fdf.reshape(24576,1)\n",
    "        W=[]\n",
    "        for i in range(len(self.eigenVectors)):\n",
    "            E = self.eigenVectors[i,:].reshape(1,24576)\n",
    "            W.append(np.dot(E,Fdf).flatten())\n",
    "        W = np.asarray(W)\n",
    "        return W\n",
    "    #每一帧图片的ims\n",
    "    def compute_pca(self,ims):\n",
    "        self.classes=['A','B','C','D']\n",
    "        #num=0\n",
    "        #if  self.frame==0:\n",
    "        print('initfeature')\n",
    "        #self.flag==True\n",
    "        featureVector = self.compute_feature(ims[0])\n",
    "        self.testsave[self.classes[0]]=featureVector\n",
    "            #self.frame+=1  \n",
    "        for k,v in self.testsave.items():\n",
    "            list_,list_W,list_w0=[],[],[]\n",
    "            list_mashi1,list_mashi2=[],[]\n",
    "            n = np.asarray(v)\n",
    "            for im in ims:\n",
    "                #im=self.fill_im(im)\n",
    "                #im=cv2.resize(im,(64,128))\n",
    "                featureVector = self.compute_feature(im)\n",
    "                list_W.append(featureVector)\n",
    "                n_ = n.reshape((128,2))\n",
    "                feature_ = featureVector.reshape((128,2))\n",
    "                list_.append(self.compute_Maha(n_,feature_))\n",
    "                #self.frame+=1\n",
    "                n =np.asarray(featureVector)*0.5+n*0.5#根据三比七的比重进行调节\n",
    "                #self.testsave[self.classes[0]]=n\n",
    "                list_w0.append(n)\n",
    "        #print('\\r %d'%self.frame,end ='')\n",
    "        for W in list_W:\n",
    "            self.createNewFace(W,self.frame)\n",
    "            self.frame+=1\n",
    "        return list_ , list_W ,list_w0\n",
    "    def createNewFace(self,W,num):\n",
    "        # Start with the mean image\n",
    "        output = self.mean.reshape(64,128,3)\n",
    "        w=W.tolist()\n",
    "        # Add the eigen faces with the weights\n",
    "        for i in range(0, self.NUM_EIGEN_FACES):\n",
    "            '''\n",
    "            OpenCV does not allow slider values to be negative. \n",
    "            So we use weight = sliderValue - MAX_SLIDER_VALUE / 2\n",
    "            ''' \n",
    "            #sliderValues[i] = cv2.getTrackbarPos(\"Weight\" + str(i), \"Trackbars\");\n",
    "            weight = w[i]\n",
    "            e = self.eigenVectors[i].reshape(64,128,3)\n",
    "            output = np.add(output, e * weight)\n",
    "\n",
    "        # Display Result at 2x size\n",
    "        #output = cv2.resize(output, (0,0), fx=2, fy=2)\n",
    "        output=output*255.0\n",
    "        cv2.imshow(\"Result\", output)\n",
    "        name=('%06d.jpg'%num)\n",
    "        cv2.imwrite('./walkingworkspace/output/'+name,output)\n",
    "        cv2.waitKey(1)\n",
    "    def pca_class_dataset(self,im,name,output_path):\n",
    "        identities = os.path.basename(name)\n",
    "        self.frame+=1\n",
    "        #testimage = self.im_creat(testimage)\n",
    "        im_ = im\n",
    "        #sz = self.images[0].shape\n",
    "        size=im_.shape\n",
    "        im_ = np.float32(im_)/255.0\n",
    "        Fim = im_.flatten()\n",
    "        Fmean = self.mean.reshape(Fim.shape)\n",
    "        Fdf = Fim-Fmean\n",
    "        Fdf= Fdf.reshape(24576,1)\n",
    "        W=[]\n",
    "        for i in range(len(self.eigenVectors)):\n",
    "            E = self.eigenVectors[i,:].reshape(1,24576)\n",
    "            #print(self.eigenVectors[i,:].reshape(1,24576).T.shape)\n",
    "            W.append(np.dot(E,Fdf).flatten())\n",
    "        W = np.asarray(W)\n",
    "        #print(W)\n",
    "        #cv2.imshow('build',build[0:])\n",
    "        name = ('/%s'%identities)\n",
    "        name = name.replace('jpg','npy')\n",
    "        np.save(output_path+name,W)\n",
    "        #cv2.imwrite(name+'.jpg',im)\n",
    "        cv2.waitKey(1)\n",
    "    def test_add_lib(self,im_,id_):\n",
    "        list_distance,list_feature=[],[]\n",
    "        featureVector = self.compute_feature(im_)\n",
    "        for k,v in list(self.save.items()):\n",
    "            if k=='000'and  k!=id_:\n",
    "                continue\n",
    "            if v['passage']==True:\n",
    "                n=v['center']\n",
    "                distance = self.compute_cos(n,featureVector)\n",
    "                if distance <self.dth:\n",
    "                    list_distance.append(distance)\n",
    "                    list_feature.append(featureVector)\n",
    "        if len(list_distance)!=0:\n",
    "                min_distance_feature = list_feature[list_distance.index(min(list_distance))]\n",
    "                center = (min_distance_feature*0.5)+(featureVector*0.5)\n",
    "                self.save[id_]['center'] =center\n",
    "                self.check_lib_add(min_distance_feature,id_,bbox)\n",
    "                                        \n",
    "    def test(self,path):\n",
    "        list_val1,list_val2,list_test1,list_test2,list_test1_loss,list_test2_loss=[],[],[],[],[],[]\n",
    "        for name in glob.glob(path+'/*'):\n",
    "            if name.find('xia001')!=-1:\n",
    "                for im_name1 in glob.glob(name+'/*.jpg'):\n",
    "                    list_val1.append(cv2.imread(im_name1))\n",
    "            if name.find('li002')!=-1:\n",
    "                for im_name2 in glob.glob(name+'/*.jpg'):\n",
    "                    list_val2.append(cv2.imread(im_name2))\n",
    "                    \n",
    "        featureVector = self.compute_feature(list_val1[0])\n",
    "        self.add_lib(featureVector,[],1,passage=True)\n",
    "        featureVector = self.compute_feature(list_val2[0])\n",
    "        self.add_lib(featureVector,[],1,passage=True)\n",
    "        for im_ in list_val1:\n",
    "            cv2.imshow('in',im_)\n",
    "            self.test_add_lib(im_,'001')\n",
    "            self.kalman.append(KalmanFilter())\n",
    "            cv2.waitKey(1)\n",
    "        for im_ in list_val2:\n",
    "            cv2.imshow('in',im_)\n",
    "            self.test_add_lib(im_,'002') \n",
    "            self.kalman.append(KalmanFilter())\n",
    "            cv2.waitKey(1)\n",
    "            \n",
    "        center =self.save['001']['lib'][0]\n",
    "        for lib_ in self.save['001']['lib']:\n",
    "            center=center*0.5+lib_*0.5\n",
    "        #self.save[k]['lib'].pop(0)\n",
    "        self.passage_off('001',center)\n",
    "        \n",
    "        center =self.save['002']['lib'][0]\n",
    "        for lib_ in self.save['002']['lib']:\n",
    "            center=center*0.5+lib_*0.5\n",
    "        #self.save[k]['lib'].pop(0)\n",
    "        self.passage_off('002',center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "efficientdet + PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading images from ./walkingworkspace/train2020...1982 files read.\n"
     ]
    }
   ],
   "source": [
    "HumanMaxNumber=4\n",
    "size=(640,480)\n",
    "E = efficientdet(0 ,640,HumanMaxNumber,dir_path='./walkingworkspace/train2020',NUM_EIGEN_FACES = 256,test=False)\n",
    "#E = efficientdet(0 ,512,HumanMaxNumber,dir_path='./datasets/multi-query',NUM_EIGEN_FACES = 256)\n",
    "#从新构建models\n",
    "#E.init_PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335.0\n",
      "find id 001\n",
      "find id 002\n",
      "001 lib len :14001\n",
      "001 lib len :15001\n",
      "001 lib len :16001\n",
      "001 lib len :17001\n",
      "001 lib len :18001\n",
      "001 lib len :19001\n",
      "001 lib len :20001\n",
      "001 lib len :21001\n",
      "001 lib len :22001\n",
      "001 lib len :23001\n",
      "002 lib len :15002\n",
      "002 lib len :16001\n",
      "001 lib len :25002\n",
      "002 lib len :17001\n",
      "002 lib len :74001\n",
      "001 lib len :81001\n",
      "002 lib len :89002\n",
      "002 lib len :90001\n",
      "001 lib len :83002\n",
      "002 lib len :91001\n",
      "002 lib len :109"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-e8c14a2a953e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0mE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mnum\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-6e246c64c0a6>\u001b[0m in \u001b[0;36mdetector\u001b[1;34m(self, im)\u001b[0m\n\u001b[0;32m     78\u001b[0m                               \u001b[0manchors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mregression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m                               \u001b[0mregressBoxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclipBoxes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m                               self.threshold, self.iou_threshold)\n\u001b[0m\u001b[0;32m     81\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minvert_affine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframed_metas\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mbbox_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mori_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\CrossvideoREID\\Myreid_EfficientDet\\utils\\utils.py\u001b[0m in \u001b[0;36mpostprocess\u001b[1;34m(x, anchors, regression, classification, regressBoxes, clipBoxes, threshold, iou_threshold)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manchors_nms_idx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mscores_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassification_per\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manchors_nms_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mboxes_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformed_anchors_per\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0manchors_nms_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "E.init_video()\n",
    "#videopath='./V/liandxia.mp4'\n",
    "#videopath='./testvideo/two_rotation.mp4'\n",
    "videopath='./testvideo/test_kelman.mp4'\n",
    "#videopath='./walkingvideo/two_rotationwalking.mp4'\n",
    "#videopath=0\n",
    "video = cv2.VideoCapture(videopath)\n",
    "video_fps = video.get(7)\n",
    "print(video_fps)\n",
    "num=0\n",
    "for i in range(int(video_fps-1)):\n",
    "#t=time.time()\n",
    "\n",
    "#while True:\n",
    "#    if (time.time()-t)>=5:\n",
    "#        if (time.time()-t)>=300:\n",
    "#            break \n",
    "        ret,im = video.read()\n",
    "        #im_=cv2.flip(im,0)\n",
    "        if ret:\n",
    "            if num%1==0:\n",
    "                E.detector(cv2.resize(im,size))\n",
    "        num+=1\n",
    "cv2.destroyAllWindows() \n",
    "E.out.release()\n",
    "video.release()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCATEST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HumanMaxNumber=4\n",
    "size=(640,480)\n",
    "E = efficientdet(0 ,640,HumanMaxNumber,dir_path='./walkingworkspace/train2020',NUM_EIGEN_FACES = 256,test=True)\n",
    "#E.init_video()\n",
    "videopath='./testvideo/test_kelman.mp4'\n",
    "#videopath=0\n",
    "video = cv2.VideoCapture(videopath)\n",
    "video_fps = video.get(7)\n",
    "print(video_fps)\n",
    "num=0\n",
    "for i in range(int(video_fps-1)):\n",
    "#t=time.time()\n",
    "#while True:\n",
    "#    if (time.time()-t)>=5:\n",
    "#        if (time.time()-t)>=300:\n",
    "#            break \n",
    "        ret,im = video.read()\n",
    "        #im_=cv2.flip(im,0)\n",
    "        if ret:\n",
    "            if num%1==0:\n",
    "                E.detector(cv2.resize(im,size))\n",
    "        num+=1\n",
    "cv2.destroyAllWindows() \n",
    "E.out.release()\n",
    "video.release()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.save('./kalman.npy',[E.save])\n",
    "print(E.save)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_test_002_true,num_test_002_false,num_test_001_true,num_test_001_false=0,0,0,0\n",
    "path = './walkingworkspace/test2020'\n",
    "list_label,list_li,list_xia,list_other=[],[],[],[]\n",
    "for name in glob.glob(path+'/LI/*.jpg'):\n",
    "    list_li.append(os.path.basename(name))\n",
    "for name in glob.glob(path+'/XI/*.jpg'):\n",
    "    list_xia.append(os.path.basename(name))\n",
    "for name in glob.glob(path+'/other/*.jpg'):\n",
    "    list_other.append(os.path.basename(name))\n",
    "    \n",
    "for name in glob.glob('./walkingworkspace/action/*.jpg'):\n",
    "    name = os.path.basename(name) \n",
    "    if name in list_other:\n",
    "        list_label.append('003')\n",
    "    if name in list_xia:\n",
    "        list_label.append('001')   \n",
    "    if name in list_li:\n",
    "        list_label.append('002')\n",
    "\n",
    "list_label=list_label[(len(list_label)-len(E.label_test)):len(list_label)]\n",
    "print(len(E.label_test),len(list_label))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_label,label in  zip(E.label_test,list_label):\n",
    "    if label=='001':\n",
    "        if test_label==label:\n",
    "            num_test_001_true+=1\n",
    "        else:\n",
    "            num_test_001_false+=1\n",
    "    if label=='002':\n",
    "        if test_label==label:\n",
    "            num_test_002_true+=1\n",
    "        else:\n",
    "            num_test_002_false+=1\n",
    "print('the 001 C:%.2f'%(num_test_001_true/(num_test_001_true+num_test_001_false)))\n",
    "print('the 002 C:%.2f'%(num_test_002_true/(num_test_002_true+num_test_002_false)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA testdata rebuild"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HumanMaxNumber=2\n",
    "#size=(512,384)\n",
    "#E = efficientdet(0 ,512,HumanMaxNumber)\n",
    "#ids_bboxes = {}\n",
    "pca=PCA(dir_path='./walkingworkspace/train2020',NUM_EIGEN_FACES = 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#E.threadStart()\n",
    "#videopath='./testvideo/mc1.mp4'\n",
    "im_path='./walkingworkspace/action/'\n",
    "list_im=[]\n",
    "for im_name in glob.glob(im_path+'*.jpg'):\n",
    "    list_im.append(im_name.replace('\\\\','/'))\n",
    "list_W=[]\n",
    "list_=[]\n",
    "\n",
    "print(len(list_im))\n",
    "list_im1,list_im2,list_im3,list_im4=[],[],[],[]\n",
    "for im_name in list_im:  \n",
    "    #print(im_name)\n",
    "    im = cv2.imread(im_name)\n",
    "    im=cv2.resize(im,(64,128))\n",
    "    #if im_name.find('A')!=-1: \n",
    "    list_im1.append(im)\n",
    "    #if im_name.find('B')!=-1:\n",
    "    #    list_im2.append(im)\n",
    "    #if im_name.find('C')!=-1:\n",
    "        #list_im3.append(im)\n",
    "    #if im_name.find('D')!=-1:\n",
    "        #list_im4.append(im)\n",
    "distance1,w1,wcenter1= pca.compute_pca(list_im1)\n",
    "#pca=PCA(dir_path='./test_colors/train',basepath = './test_colors/source',NUM_EIGEN_FACES = 64)\n",
    "#distance2,w2,wcenter2= pca.compute_pca(list_im2)\n",
    "#data3= pca.compute_pca(list_im3)\n",
    "#data4= pca.compute_pca(list_im4)\n",
    "print(distance1)\n",
    "#print(len(distance2))\n",
    "#print(data2)\n",
    "#    X,Y,Z=data\n",
    "'''if im_name.find('A')!=-1:\n",
    "    set_A_x.append(X)\n",
    "    set_A_y.append(Y)\n",
    "    set_A_z.append(Z)\n",
    "if im_name.find('B')!=-1:\n",
    "    set_B_x.append(X)\n",
    "    set_B_y.append(Y)\n",
    "    set_B_z.append(Z)'''\n",
    "'''if im_name.find('O')!=-1:\n",
    "    set_O_x.append(X)\n",
    "    set_O_y.append(Y)\n",
    "    set_O_z.append(Z)\n",
    "if im_name.find('C')!=-1:\n",
    "    set_C_x.append(X)\n",
    "    set_C_y.append(Y)\n",
    "    set_C_z.append(Z)\n",
    "if im_name.find('E')!=-1:\n",
    "    set_E_x.append(X)\n",
    "    set_E_y.append(Y)\n",
    "    set_E_z.append(Z)  ''' \n",
    "#    list_W.append(W)\n",
    "    #pca.pca_class_dataset(im,im_name,output_path)\n",
    "#pca.test_meandyou()\n",
    "cv2.destroyAllWindows() \n",
    "#print(len(set_A_x),len(set_A_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#data1.remove(max(data1))\n",
    "#data1.remove(min(data1))\n",
    "#data2.remove(max(data2))\n",
    "#data2.remove(min(data2))\n",
    "max_jiawei = max(data1)\n",
    "max_xiaobo = max(data2)\n",
    "print(max_jiawei,max_xiaobo)\n",
    "max_jiawei,max_xiaobo = float(max_jiawei),float(max_xiaobo)\n",
    "print((max_jiawei+max_xiaobo)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.ylabel('I am distance')\n",
    "plt.xlabel('I am id')\n",
    "#plt.legend([L1,L2],['A','B'],loc='upper right')\n",
    "x1=range(0,len(distance1))\n",
    "#x2=range(0,len(distance2))\n",
    "x3=range(0,50)\n",
    "x4=range(0,50)\n",
    "plt.plot(x1,distance1,label='A',linewidth=3,color='r',marker='o',\n",
    "markerfacecolor='r',markersize=12)\n",
    "plt.show()\n",
    "#plt.plot(x2,distance2,label='B',linewidth=3,color='r',marker='o',\n",
    "#markerfacecolor='g',markersize=12)\n",
    "#plt.show()\n",
    "#plt.plot(x1,data3,label='C',linewidth=3,color='r',marker='o',\n",
    "#markerfacecolor='b',markersize=12)\n",
    "#plt.plot(x2,data4,label='D',linewidth=3,color='r',marker='o',\n",
    "#markerfacecolor='y',markersize=12)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#语义分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "self.net = fcn_resnet101(pretrained=True,progress=False).cuda()\n",
    "_ = self.net.eval()\n",
    "def fcn_mask(self,im):\n",
    "    im = cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\n",
    "    input_batch = torch.tensor(im,dtype=torch.float32).cuda()\n",
    "    input_batch = input_batch.permute(2,0,1)\n",
    "    input_batch = 2*input_batch/255.0-1.0\n",
    "    input_batch = input_batch.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        output = net(input_batch)['out'][0]\n",
    "    output_predictions = output.argmax(0).cpu()\n",
    "    output_predictions = np.where(output_predictions[:]==15,1,0)\n",
    "    return output_predictions\n",
    "\n",
    "def create_mask(self,im_bbox):\n",
    "    im_fcn = self.fcn_mask(im_bbox)\n",
    "    re = np.zeros(im_bbox.shape)*255\n",
    "    #im_fcn_=np.zeros((im_fcn.shape[0],im_fcn.shape[1],3))\n",
    "    re[:,:,0]=re[:,:,1]=re[:,:,2]=im_fcn\n",
    "    #re = np.where(re.all()!=[0,0,0], [0,0,0], [255,255,255])\n",
    "    re = re*im_bbox\n",
    "    re = re.astype(np.uint8)\n",
    "    cv2.imshow('mask',re)\n",
    "    return re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure()\n",
    "plt = fig.gca(projection='3d')\n",
    "plt.plot(set_A_x,set_A_y,set_A_z,label='Frist line',linewidth=3,color='r',marker='o',\n",
    "markerfacecolor='w',markersize=12)\n",
    "plt.plot(set_B_x,set_B_y,set_B_z,label='Frist line',linewidth=3,color='r',marker='o',\n",
    "markerfacecolor='b',markersize=12)\n",
    "plt.plot(set_O_x,set_O_y,set_O_z,label='Frist line',linewidth=3,color='r',marker='o',\n",
    "markerfacecolor='g',markersize=12)\n",
    "plt.plot(set_C_x,set_C_y,set_C_z,label='Frist line',linewidth=3,color='r',marker='o',\n",
    "markerfacecolor='y',markersize=12)\n",
    "plt.plot(set_C_x,set_C_y,set_C_z,label='Frist line',linewidth=3,color='r',marker='o',\n",
    "markerfacecolor='y',markersize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,ConcatDataset\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "device = torch.device(\"cuda:0,1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Dataset):\n",
    "    def __init__(self, fpath_peoject=[]):\n",
    "        \n",
    "        self.labels,self.images = [],[]\n",
    "        for i in fpath_peoject:\n",
    "            j = i[1]\n",
    "            np_im = np.load(i[0])\n",
    "            self.labels.append(j)\n",
    "            np_im = np_im.flatten()\n",
    "            #print(np_im.shape)\n",
    "            self.images.append(np_im)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return (self.images[idx],self.labels[idx])\n",
    "classes=['A','B']\n",
    "def get_data(path):\n",
    "    list_train,list_val=[],[]\n",
    "    for name in glob.glob(path+'/*.npy'):\n",
    "        name_=name.replace('\\\\','/')\n",
    "        if name_.find('A')!=-1:\n",
    "            list_train.append([name_,0])\n",
    "        if name_.find('B')!=-1:\n",
    "            list_train.append([name_,1])\n",
    "    return list_train\n",
    "train = get_data('./miniclassify/train_max')  \n",
    "test = get_data('./miniclassify/test_max') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mytraindataset = DataLoader(train)\n",
    "mytestataset = DataLoader(test)\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "trainloader = torch.utils.data.DataLoader(mytraindataset, batch_size=6,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "testloader = torch.utils.data.DataLoader(mytestataset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=0)\n",
    "print(len(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(100,1000)\n",
    "        self.fc3 = nn.Linear(1000,2)\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "net.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)\n",
    "#optimizer = optim.Adam(net.parameters(), lr=0.0001, betas=(0.9, 0.99))\n",
    "PATH='./pytorchmodel/my_max.pth'\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device),data[1].to(device)\n",
    "        #print(inputs,labels)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 6 == 0:    # print every 2000 mini-batches\n",
    "            print('\\r [%d, %5d] loss: %.4f' %\n",
    "                  (epoch + 1, i + 1, running_loss/100),end=' ')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images=images.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted.cpu() == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load(PATH))\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)),cmap=\"gray\")\n",
    "    plt.show()\n",
    "    \n",
    "dataiter = iter(trainloader)\n",
    "images,lables= dataiter.next()\n",
    "print(type(images))\n",
    "images,lables=images.to(device),lables.to(device)\n",
    "print(images.shape)\n",
    "outputs = net(images)\n",
    "print(images.shape)\n",
    "_, predicted = torch.max(outputs,1)\n",
    "print(predicted)\n",
    "imshow(torchvision.utils.make_grid(images.cpu()))\n",
    "print('Predicted: ', ' '.join('%11s' % classes[predicted[j]]\n",
    "                              for j in range(len(images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='./pytorchmodel/my_max.pth'\n",
    "mynet = torch.load(PATH)\n",
    "print(mynet['fc1.weight'].shape)\n",
    "print(mynet['fc3.weight'].shape)\n",
    "#print(mynet)\n",
    "#print(type(mynet))\n",
    "W = mynet['fc1.weight']\n",
    "W_weight=[]\n",
    "for i in range(0,100):\n",
    "    #print(W[:,i].shape)\n",
    "    W_weight.append(np.mean(abs(W[:,i]).cpu().numpy()))\n",
    "print(W_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x1=range(0,100)\n",
    "plt.plot(x1,W_weight,label='Frist line',linewidth=3,color='r',marker='o',\n",
    "markerfacecolor='blue',markersize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "\n",
    "# Number of EigenFaces\n",
    "NUM_EIGEN_FACES = 10\n",
    "\n",
    "# Maximum weight\n",
    "MAX_SLIDER_VALUE = 10\n",
    "\n",
    "# Directory containing images\n",
    "dirName = \"./pytorchdata/test\"\n",
    "\n",
    "# Read images\n",
    "images=[]\n",
    "for name in glob.glob(dirName+'/*.jpg'):\n",
    "    #print(name)\n",
    "    images.append(cv2.resize(cv2.imread(name.replace('\\\\','/')),(64,128)))\n",
    "    cv2.imshow('0-0',cv2.imread(name.replace('\\\\','/')))\n",
    "    cv2.waitKey(1)\n",
    "\n",
    "# Size of images\n",
    "sz = images[0].shape\n",
    "\n",
    "# Create data matrix for PCA.\n",
    "data = createDataMatrix(images)\n",
    "\n",
    "# Compute the eigenvectors from the stack of images created\n",
    "print(\"Calculating PCA \", end=\"...\")\n",
    "mean, eigenVectors = cv2.PCACompute(data, mean=None, maxComponents=NUM_EIGEN_FACES)\n",
    "print(mean.shape, eigenVectors.shape)\n",
    "print (\"DONE\")\n",
    "\n",
    "averageFace = mean.reshape(sz)\n",
    "print(averageFace.shape)\n",
    "cv2.imshow('averageFace',averageFace)\n",
    "eigenFaces = []; \n",
    "\n",
    "for eigenVector in eigenVectors:\n",
    "    eigenFace = eigenVector.reshape(sz)\n",
    "    eigenFaces.append(eigenFace)\n",
    "\n",
    "# Create window for displaying Mean Face\n",
    "cv2.namedWindow(\"Result\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "# Display result at 2x size\n",
    "output = cv2.resize(averageFace, (0,0), fx=2, fy=2)\n",
    "cv2.imshow(\"Result\", output)\n",
    "\n",
    "# Create Window for trackbars\n",
    "cv2.namedWindow(\"Trackbars\", cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "sliderValues = []\n",
    "\n",
    "# Create Trackbars\n",
    "for i in range(0, NUM_EIGEN_FACES):\n",
    "    sliderValues.append(MAX_SLIDER_VALUE/2)\n",
    "    cv2.createTrackbar( \"Weight\" + str(i), \"Trackbars\", int(MAX_SLIDER_VALUE), MAX_SLIDER_VALUE, createNewFace)\n",
    "\n",
    "# You can reset the sliders by clicking on the mean image.\n",
    "cv2.setMouseCallback(\"Result\", createNewFace);\n",
    "\n",
    "print('''Usage:\n",
    "Change the weights using the sliders\n",
    "Click on the result window to reset sliders\n",
    "Hit ESC to terminate program.''')\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def efficientdet_compute_pca(self,bbox_list,confidences,im):\n",
    "        #\n",
    "        # * self.notebook 记录发现新的图片的W\n",
    "        # * 保存数据的格式 self.save={'id':{'center':w,'LIB':[w1..wn]}}\n",
    "        #\n",
    "        color=[]\n",
    "        identities=[]\n",
    "        dths=[]\n",
    "        self.frame+=1\n",
    "        #维护LIB\n",
    "        self.manage_lib(self.frame)\n",
    "        if bbox_list!=[]:\n",
    "            for bbox,c in zip(bbox_list,confidences):\n",
    "                list_id,list_feature,list_distance,list_n=[],[],[],[]\n",
    "                \n",
    "                (x, y, w, h) = bbox\n",
    "                (x1, y1, x2, y2)=int(x-w/2), int(y-h/2), int(x+w/2), int(y+h/2)\n",
    "                (x,y,w,h)=(x1+x2)*0.5,(y1+y2)*0.5,abs(x1-x2),abs(y1-y2)\n",
    "                im_=im[y1:y2,x1:x2]\n",
    "                #im_ = self.fill_im(im_)\n",
    "                im_ = cv2.resize(im_,(64,128))\n",
    "                featureVector = self.compute_feature(im_)\n",
    "                distance=0\n",
    "            #所有图片与LIB对比 判断是否是新的id 跟新center\n",
    "                for k,v in list(self.save.items()):\n",
    "                    if k=='000':\n",
    "                        continue\n",
    "                    if v['passage']==True:\n",
    "                        n=v['center']\n",
    "                        distance = self.compute_cos(n,featureVector)\n",
    "                        if distance <self.dth:\n",
    "                            list_distance.append(distance)\n",
    "                            list_id.append(k)\n",
    "                            list_feature.append(featureVector)\n",
    "                            list_n.append(n)\n",
    "                #判断有匹配的则进行更新\n",
    "                if len(list_distance)!=0:\n",
    "                    min_number=list_distance.index(min(list_distance))\n",
    "                    min_distance = list_distance[min_number]\n",
    "                    min_distance_id = list_id[min_number]\n",
    "                    min_distance_feature=list_feature[min_number]\n",
    "                    min_n = list_n[min_number]\n",
    "                    #dths.append(1-min(list_distance))\n",
    "                    dths.append(c)\n",
    "                     #检查除id_me外是否与数据库里的bbox重叠\n",
    "                    ret_bbox =self.check_bboxs(bbox_list,bbox)\n",
    "                    #检查重新回来的id的frame\n",
    "                    #ret_id_frame = self.check_frame_id(min_distance_id,min_distance_feature,self.frame)\n",
    "\n",
    "                    if ret_bbox==True :#and ret_id_frame==True:\n",
    "                        center = (min_distance_feature*0.5)+(min_n*0.5)\n",
    "                        self.save[min_distance_id]['center'] =center\n",
    "                        self.save[min_distance_id]['bbox'] =bbox\n",
    "                        self.save[min_distance_id]['frame'] =self.frame\n",
    "                        color_ = self.check_lib_add(min_distance_feature,min_distance_id)\n",
    "                        color.append(color_)\n",
    "                    identities.append(min_distance_id)\n",
    "\n",
    "                #没有匹配的图片出现\n",
    "                else:\n",
    "                    #检查与本帧的bbox是否重叠 和边缘是否重叠\n",
    "                    ret_bbox =self.check_bboxs(bbox_list,bbox)\n",
    "                    #ret_id =self.check_id(bbox_list,bbox,'#')\n",
    "                    ret_lib = self.check_lib(featureVector)\n",
    "                    if ret_bbox==True and ret_lib==' ':\n",
    "                            id_ = self.add_id(featureVector,bbox,self.frame)\n",
    "                            identities.append(id_)\n",
    "                            #dths.append(1-distance)\n",
    "                            dths.append(c)\n",
    "                            color.append(0)\n",
    "                    else:\n",
    "                        color.append(0)\n",
    "                        dths.append(c)\n",
    "                        identities.append(' ')  \n",
    "            identities = self.kalman_reid(identities,bbox_list)\n",
    "        self.frame+=1\n",
    "        \n",
    "        return identities,dths,color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def efficientdet_compute_pca(self,bbox_list,confidences,im):\n",
    "        #\n",
    "        # * self.notebook 记录发现新的图片的W\n",
    "        # * 保存数据的格式 self.save={'id':{'center':w,'LIB':[w1..wn]}}\n",
    "        #\n",
    "        color=[]\n",
    "        identities=[]\n",
    "        dths=[]\n",
    "        #维护LIB\n",
    "        self.manage_lib(self.frame)\n",
    "        list_id,list_feature,list_distance,list_n=[],[],[],[]\n",
    "        if bbox_list!=[]:\n",
    "            for bbox,c in zip(bbox_list,confidences):\n",
    "                (x, y, w, h) = bbox\n",
    "                (x1, y1, x2, y2)=int(x-w/2), int(y-h/2), int(x+w/2), int(y+h/2)\n",
    "                (x,y,w,h)=(x1+x2)*0.5,(y1+y2)*0.5,abs(x1-x2),abs(y1-y2)\n",
    "                im_=im[y1:y2,x1:x2]\n",
    "                #im_ = self.fill_im(im_)\n",
    "                im_ = cv2.resize(im_,(64,128))\n",
    "                #第一帧的所有创建自己的组\n",
    "                #if self.frame==0:\n",
    "                #    featureVector = self.compute_feature(im_)\n",
    "                #    id_ = self.add_id(featureVector,bbox,self.frame)\n",
    "                #    identities.append(id_)\n",
    "                #    continue\n",
    "                #else: \n",
    "                featureVector = self.compute_feature(im_)\n",
    "                distance=0\n",
    "            #所有图片与LIB对比 判断是否是新的id 分别存储到不同的list\n",
    "                list_distance_buffer,list_id_buffer,list_feature_buffer,list_n_buffer=[],[],[],[]\n",
    "                for k,v in list(self.save.items()):\n",
    "                    if k=='000':\n",
    "                        continue\n",
    "                    n=v['center']\n",
    "                    if v['passage']==True:\n",
    "                        distance = self.compute_cos(n,featureVector)\n",
    "                        list_distance_buffer.append(distance)\n",
    "                        list_id_buffer.append(k)\n",
    "                        list_feature_buffer.append(featureVector)\n",
    "                        list_n_buffer.append(n)\n",
    "                if len(list_distance_buffer)>0 and min(list_distance_buffer)<self.dth:\n",
    "                    list_distance.append(list_distance_buffer)\n",
    "                    list_id+=list_id_buffer\n",
    "                    list_feature+=list_feature_buffer\n",
    "                    list_n+=list_n_buffer\n",
    "                else:\n",
    "                    #distance_ = min(list_distance_buffer)\n",
    "                    #检查与本帧的bbox是否重叠 和边缘是否重叠\n",
    "                    ret_bbox =self.check_bboxs(bbox_list,bbox)\n",
    "                    #ret_id =self.check_id(bbox_list,bbox,'#')\n",
    "                    ret_lib = self.check_lib(featureVector)\n",
    "                    if ret_bbox==True:\n",
    "                        if ret_lib==' ':\n",
    "                            id_ = self.add_id(featureVector,bbox,self.frame)\n",
    "                            identities.append(id_)\n",
    "                            #dths.append(1-distance_)\n",
    "                            dths.append(c)\n",
    "                            color.append(0)\n",
    "                        else:\n",
    "                            color.append(0)\n",
    "                            dths.append(c)\n",
    "                            #self.paassag_on(featureVector,bbox,self.frame,ret_lib)\n",
    "                            identities.append(' ')\n",
    "                    else:\n",
    "                        color.append(0)\n",
    "                        dths.append(c)\n",
    "                        identities.append(' ')  \n",
    "\n",
    "            if len(list_distance)>0 and len(bbox_list)>0:\n",
    "                list_center,list_min_n,list_identities=[],[],[]\n",
    "                np_distance=np.asarray(list_distance)\n",
    "                #print(np_distance)\n",
    "                #print(self.save.keys())\n",
    "                list_Hungary = self.computer_Hungary(np_distance)\n",
    "                #print(list_Hungary)\n",
    "                for number in list_Hungary:\n",
    "                    #number = number*0.00001\n",
    "                    min_distance = np_distance.flatten().tolist().index(number)\n",
    "                    list_identities.append(list_id[min_distance])\n",
    "                    list_center.append(list_feature[min_distance])\n",
    "                    list_min_n.append(list_n[min_distance])\n",
    "\n",
    "                #检查重新回来的id的frame\n",
    "                #ret_id_frame = self.check_frame_id(min_distance_id,min_distance_feature,self.frame)\n",
    "                for bbox,conter,min_n,id_,distance_ in zip(bbox_list,list_center,list_min_n,list_identities,list_Hungary):\n",
    "                     #检查除id_me外是否与数据库里的bbox重叠\n",
    "                    #if distance_<self.dth:\n",
    "                        ret_bbox =self.check_bboxs(bbox_list,bbox)\n",
    "                        if ret_bbox==True :#and ret_id_frame==True:\n",
    "                            center_ = (conter*0.5)+(min_n*0.5)\n",
    "                            self.save[id_]['center'] =center_\n",
    "                            self.save[id_]['bbox'] =bbox\n",
    "                            self.save[id_]['frame'] =self.frame\n",
    "                            color_ = self.check_lib_add(conter,id_,bbox)\n",
    "                            color.append(color_)\n",
    "                            dths.append(1-distance_)\n",
    "                            identities.append(id_)\n",
    "                        else:\n",
    "                            #self.save[id_]['bbox'] =bbox\n",
    "                            self.save[id_]['frame'] =self.frame\n",
    "                            color_ = self.check_lib_add(conter,id_,bbox)\n",
    "                            color.append(color_)\n",
    "                            dths.append(1-distance_)\n",
    "                            identities.append(' ')\n",
    "                    \n",
    "        self.frame+=1\n",
    "        #保存id\n",
    "        self.old_id=identities\n",
    "        return identities,dths,color"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
